{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319f9c82",
   "metadata": {},
   "source": [
    "# Colab Training for SOC-LLM (QLoRA)\n",
    "Train in short sessions, push LoRA adapters to Hugging Face Hub, and resume later.\n",
    "**Remember:** Use the same `config/axo.yaml` everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eade944",
   "metadata": {},
   "source": [
    "## 1) Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U datasets transformers accelerate peft trl bitsandbytes scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd466a",
   "metadata": {},
   "source": [
    "## 1.1) Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef392a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "print(\"Python:\", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941eb9f",
   "metadata": {},
   "source": [
    "## 2) Authenticate to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943da8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "HF_TOKEN = getpass(\"Enter your Hugging Face write token: \")\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "HF_REPO = \"com-otu/soc-llm-lora\"  # change me\n",
    "os.environ[\"HF_REPO\"] = HF_REPO\n",
    "print(\"HF env set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c9439",
   "metadata": {},
   "source": [
    "## 3) Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"data/training/classification/sieve_prepped\")  # adjust if needed\n",
    "\n",
    "TRAIN_JSONL = DATA_DIR / \"train.instruction.jsonl\"\n",
    "VAL_JSONL   = DATA_DIR / \"val.instruction.jsonl\"\n",
    "TEST_JSONL  = DATA_DIR / \"test.instruction.jsonl\"\n",
    "LABELS_TXT  = DATA_DIR / \"labels.txt\"\n",
    "\n",
    "assert TRAIN_JSONL.exists() and VAL_JSONL.exists()\n",
    "labels = [l.strip() for l in open(LABELS_TXT, \"r\", encoding=\"utf-8\").read().splitlines()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392f50e",
   "metadata": {},
   "source": [
    "## 4) Train (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"  # change if needed\n",
    "MAX_SEQ_LEN = 1024\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "GRAD_ACCUM = 32\n",
    "PER_DEVICE_BATCH = 2\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "SEED = 42\n",
    "\n",
    "train_jsonl = str(Path(\"sieve_prepped\") / \"train.instruction.jsonl\")\n",
    "val_jsonl   = str(Path(\"sieve_prepped\") / \"val.instruction.jsonl\")\n",
    "test_jsonl  = str(Path(\"sieve_prepped\") / \"test.instruction.jsonl\")\n",
    "labels_txt  = str(Path(\"sieve_prepped\") / \"labels.txt\")\n",
    "\n",
    "#Load Dataset\n",
    "raw_ds = load_dataset(\"json\", data_files={\"train\": TRAIN_JSONL, \"validation\": VAL_JSONL, \"test\": TEST_JSONL})\n",
    "\n",
    "def to_text(example):\n",
    "    return {\"text\": f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nAnswer: {example['output']}\"}\n",
    "\n",
    "ds = raw_ds.map(to_text, remove_columns=raw_ds[\"train\"].column_names)\n",
    "\n",
    "#Tokenizer and 4Bit Qlora\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_cfg, device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "\n",
    "#Add Lora Adapter using PEFT\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Trainer config\n",
    "train_cfg = SFTConfig(\n",
    "    output_dir=\"sieve-llm-qlora\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    bf16=True,\n",
    "    seed=SEED,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "#Start Training\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    args=train_cfg\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"sieve-llm-qlora\")\n",
    "print(\"Saved QLoRA adapter to sieve-llm-qlora/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee573a77",
   "metadata": {},
   "source": [
    "## 5) Validation: Constrained Mapping to Labels + Macro-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"sieve-llm-2cat\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=8,\n",
    "    temperature=0.0,   # deterministic\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "def classify_one(logline: str) -> str:\n",
    "    prompt = f\"Instruction: Classify the SIEM event type for this log line.\\nInput: {logline}\\nAnswer:\"\n",
    "    out = gen(prompt)[0][\"generated_text\"]\n",
    "    tail = out.split(\"Answer:\", 1)[-1].strip().lower()\n",
    "\n",
    "    # exact match first\n",
    "    for lab in labels:\n",
    "        if lab.lower() in tail:\n",
    "            return lab\n",
    "\n",
    "    # remove punctuation and compare tokens\n",
    "    token = re.sub(r\"[^a-z0-9\\-]+\", \" \", tail).strip().split()\n",
    "    token = token[0] if token else \"\"\n",
    "    if token:\n",
    "        import difflib\n",
    "        guess = difflib.get_close_matches(token, [l.lower() for l in labels], n=1, cutoff=0.0)\n",
    "        if guess:\n",
    "            # map back to original label casing\n",
    "            idx = [l.lower() for l in labels].index(guess[0])\n",
    "            return labels[idx]\n",
    "    # final fallback: first label\n",
    "    return labels[0]\n",
    "\n",
    "# Build X/y from the *original* validation jsonl (avoid leakage via 'text')\n",
    "import json\n",
    "val_rows = [json.loads(l) for l in open(VAL_JSONL, \"r\", encoding=\"utf-8\").read().splitlines()]\n",
    "y_true = [r[\"output\"] for r in val_rows]\n",
    "y_pred = [classify_one(r[\"input\"]) for r in val_rows]\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(\"Macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "#!python scripts/eval_soc_json.py --gold data/val.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f2ce0",
   "metadata": {},
   "source": [
    "## 6) Push LoRA checkpoint to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_folder, HfApi\n",
    "\n",
    "PUSH_ADAPTER = False         # set True to push LoRA adapter\n",
    "PUSH_DATASET = False         # set True to push JSONL as a dataset\n",
    "HF_USERNAME = \"com-otu\"\n",
    "\n",
    "if PUSH_ADAPTER:\n",
    "    adapter_repo = f\"{HF_USERNAME}/sieve-llm-qlora-adapter\"\n",
    "    create_repo(adapter_repo, private=True, exist_ok=True)\n",
    "    upload_folder(repo_id=adapter_repo, folder_path=\"sieve-llm-qlora\", path_in_repo=\".\")\n",
    "    print(\"Adapter pushed:\", f\"https://huggingface.co/{adapter_repo}\")\n",
    "\n",
    "if PUSH_DATASET:\n",
    "    ds_repo = f\"{HF_USERNAME}/sieve-2cat-jsonl\"\n",
    "    api = HfApi()\n",
    "    api.create_repo(ds_repo, repo_type=\"dataset\", private=True, exist_ok=True)\n",
    "    upload_folder(repo_id=ds_repo, repo_type=\"dataset\", folder_path=str(DATA_DIR), path_in_repo=\".\")\n",
    "    print(\"Dataset pushed:\", f\"https://huggingface.co/datasets/{ds_repo}\")\n",
    "#!axolotl push-lora config/axo.yaml --repo $HF_REPO --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305bec0a",
   "metadata": {},
   "source": [
    "## 6) Resume training from last checkpoint (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be063736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume:\n",
    "!axolotl train config/axo.yaml --resume_from_checkpoint $HF_REPO\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
